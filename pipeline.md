# Project Pipeline: Hand Gesture Presentation Controller
**Course:** Computer Vision (BINUS University)
**Objective:** Control presentation slides ("Next", "Previous") using hand gestures via webcam.
[cite_start]**Constraint Checklist:** - [x] Conventional Models Only (No CNN/YOLO) [cite: 12]
- [x] [cite_start]Feature Extraction Implementation [cite: 12]
- [x] [cite_start]Custom Dataset (Pascal VOC format) [cite: 15]

---

## Phase 1: Data Pre-processing (The Foundation)
**Goal:** Convert raw Roboflow annotations into a dataset usable by conventional classifiers.
**Context:** Standard Classifiers (SVM/KNN) generally require cropped images of the subject, not full-frame scenes with bounding box coordinates.

1.  **Parse Pascal VOC XML:**
    * [cite_start]Read the `.xml` files generated by Roboflow[cite: 15].
    * Extract `xmin`, `ymin`, `xmax`, `ymax` coordinates.
2.  **Crop & Clean:**
    * Use the coordinates to crop the hand out of the original image.
    * **Crucial Step:** Discard the background information to prevent the model from learning the "room" instead of the "hand."
3.  **Directory Restructuring:**
    * Save cropped images into a clean folder structure:
        ```text
        /dataset_final
            /next      (contains ~cropped hand images)
            /previous  (contains ~cropped hand images)
        ```
    * [cite_start]*Note:* Ensure the augmented versions (noise, blur, grayscale) are treated the same way[cite: 14].

---

## Phase 2: Feature Extraction (LO 2 & LO 3)
[cite_start]**Goal:** Convert images into numerical vectors using "Feature Detection" as required[cite: 12].

1.  **Preprocessing per Image:**
    * **Grayscale:** Convert all images to 1 channel.
    * **Resize:** Resize all crops to a fixed dimension (e.g., `64x128` pixels). This is mandatory because SVMs require fixed-length input vectors.
2.  **Descriptor Selection: HOG (Histogram of Oriented Gradients):**
    * *Why HOG:* It excels at capturing shape and edge orientation, which is exactly what distinguishes a "Next" gesture from a "Previous" gesture.
    * *Parameters:* Define `pixels_per_cell` (e.g., 8x8) and `cells_per_block` (e.g., 2x2).
3.  **Vectorization:**
    * Flatten the HOG features into a 1D array. This array is the "fingerprint" of the gesture.

---

## Phase 3: Model Training (LO 4)
[cite_start]**Goal:** Train a "Conventional Model" to classify the HOG vectors[cite: 12].

1.  **Split Data:**
    * 80% Training / 20% Testing.
    * [cite_start]*Requirement:* Used for the "Methodology" and "Results" section of the report[cite: 6].
2.  **Algorithm Selection: SVM (Support Vector Machine):**
    * *Kernel:* Linear or RBF (Radial Basis Function).
    * *Why:* SVM is the industry standard for HOG-based classification.
3.  **Training Loop:**
    * Input: HOG Vectors + Labels (0 for Next, 1 for Previous).
    * Output: Trained Model File (`gesture_model.pkl`).
4.  **Evaluation:**
    * Generate a **Confusion Matrix**.
    * Calculate Accuracy, Precision, and Recall.

---

## Phase 4: Application Development (LO 4)
[cite_start]**Goal:** Build the functional CV application for the demo video[cite: 5, 7].

1.  **The "Static ROI" Strategy:**
    * Since object detection (finding the hand) without Deep Learning is difficult/unreliable, use a **Region of Interest (ROI)**.
    * Draw a static GREEN BOX on the webcam feed.
    * Instruction to user: "Put your hand inside the box to control slides."
2.  **Inference Pipeline (Loop):**
    * Capture Frame.
    * Extract the ROI (the pixels inside the green box).
    * Preprocess (Gray -> Resize -> HOG).
    * **Predict:** Pass to SVM model.
3.  **Control Logic:**
    * If `Prediction == Next` AND `Confidence > Threshold`:
        * [cite_start]Trigger `pyautogui.press('right')`[cite: 12].
    * If `Prediction == Previous` AND `Confidence > Threshold`:
        * [cite_start]Trigger `pyautogui.press('left')`[cite: 12].
    * *Debounce:* Add a cooldown timer (e.g., 2 seconds) so one gesture doesn't skip 50 slides at once.

---

## Phase 5: Reporting & Deliverables
[cite_start]**Goal:** Satisfy the Written Report and Demo Video requirements[cite: 5, 6, 7].

1.  **Written Report (10-15 Pages):**
    * **Methodology:** Document the HOG parameters and SVM settings.
    * [cite_start]**Data:** Explain the Roboflow augmentation (noise, blur, etc.)[cite: 14].
    * **Results:** Paste the Confusion Matrix. Discuss why the model fails (e.g., lighting changes).
2.  **Demo Video (< 5 mins):**
    * Show the "Green Box" approach.
    * Demonstrate live slide switching.
    * Explain that no Deep Learning was used, complying with constraints.