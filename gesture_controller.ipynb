{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Production-Ready Hand Gesture Presentation Controller\n",
    "\n",
    "Features:\n",
    "- Real-time webcam capture with OpenCV\n",
    "- Hand gesture detection using trained ML model\n",
    "- Automatic presentation control via keyboard simulation\n",
    "- Confidence-based filtering and cooldown mechanism\n",
    "- Graceful error handling and resource cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imports_header",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pyautogui\n",
    "from skimage.feature import hog\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "class_header",
   "metadata": {},
   "source": [
    "## GestureController Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "gesture_controller_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GestureController:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: Path,\n",
    "        confidence_threshold: float = 0.65,\n",
    "        disappear_frames: int = 5,\n",
    "        roi_bounds: Tuple[int, int, int, int] = (0, 0, 640, 480)\n",
    "    ):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.disappear_frames = disappear_frames\n",
    "        self.roi_bounds = roi_bounds\n",
    "        \n",
    "        self.model = None\n",
    "        self.hog_params = None\n",
    "        self.target_size = None\n",
    "        self.class_names = None\n",
    "        \n",
    "        self.cap = None\n",
    "        self.frame_count = 0\n",
    "        self.last_triggered_label = None\n",
    "        self.no_detection_count = 0\n",
    "        self.can_trigger = True\n",
    "        self.hand_bbox = None\n",
    "        \n",
    "        self._load_model()\n",
    "        self._init_camera()\n",
    "        \n",
    "    def _load_model(self) -> None:\n",
    "        try:\n",
    "            if not self.model_path.exists():\n",
    "                raise FileNotFoundError(f\"Model file not found: {self.model_path}\")\n",
    "            \n",
    "            if self.model_path.suffix == '.pkl':\n",
    "                logger.info(f\"Loading SVM model from {self.model_path}\")\n",
    "                bundle = joblib.load(self.model_path)\n",
    "                self.model = bundle.get(\"model\")\n",
    "                self.hog_params = bundle.get(\"hog_params\")\n",
    "                self.target_size = tuple(bundle.get(\"target_size\", (128, 128)))\n",
    "                self.class_names = self.model.classes_.tolist()\n",
    "                \n",
    "            elif self.model_path.suffix == '.h5':\n",
    "                logger.info(f\"Loading Keras model from {self.model_path}\")\n",
    "                try:\n",
    "                    from tensorflow import keras\n",
    "                    self.model = keras.models.load_model(self.model_path)\n",
    "                    \n",
    "                    labels_path = self.model_path.parent / \"labels.txt\"\n",
    "                    if labels_path.exists():\n",
    "                        with open(labels_path, 'r') as f:\n",
    "                            self.class_names = [line.strip() for line in f if line.strip()]\n",
    "                    else:\n",
    "                        self.class_names = [\"next\", \"previous\"]\n",
    "                    \n",
    "                    input_shape = self.model.input_shape\n",
    "                    self.target_size = (input_shape[1], input_shape[2])\n",
    "                    \n",
    "                except ImportError:\n",
    "                    raise ImportError(\"TensorFlow/Keras not installed. Install with: pip install tensorflow\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model format: {self.model_path.suffix}\")\n",
    "            \n",
    "            logger.info(f\"Model loaded successfully. Classes: {self.class_names}\")\n",
    "            logger.info(f\"Target input size: {self.target_size}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _init_camera(self) -> None:\n",
    "        try:\n",
    "            self.cap = cv2.VideoCapture(0)\n",
    "            \n",
    "            if not self.cap.isOpened():\n",
    "                raise RuntimeError(\"Failed to open webcam\")\n",
    "            \n",
    "            self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "            self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "            \n",
    "            logger.info(\"Webcam initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize camera: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _preprocess_roi(self, frame: np.ndarray) -> np.ndarray:\n",
    "        x0, y0, x1, y1 = self.roi_bounds\n",
    "        \n",
    "        h, w = frame.shape[:2]\n",
    "        x0, x1 = max(0, x0), min(w, x1)\n",
    "        y0, y1 = max(0, y0), min(h, y1)\n",
    "        \n",
    "        roi_frame = frame[y0:y1, x0:x1]\n",
    "        \n",
    "        if roi_frame.size == 0:\n",
    "            raise ValueError(\"Invalid ROI bounds - extracted region is empty\")\n",
    "        \n",
    "        if self.model_path.suffix == '.pkl' and self.hog_params:\n",
    "            gray = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, self.target_size)\n",
    "            normalized = resized.astype(np.float32) / 255.0\n",
    "            descriptor = hog(normalized, **self.hog_params)\n",
    "            return descriptor.reshape(1, -1)\n",
    "        \n",
    "        elif self.model_path.suffix == '.h5':\n",
    "            resized = cv2.resize(roi_frame, self.target_size)\n",
    "            normalized = resized.astype(np.float32) / 255.0\n",
    "            \n",
    "            if len(normalized.shape) == 2:\n",
    "                normalized = np.expand_dims(normalized, axis=-1)\n",
    "            \n",
    "            return np.expand_dims(normalized, axis=0)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError(f\"Preprocessing not implemented for {self.model_path.suffix}\")\n",
    "    \n",
    "    def _predict(self, features: np.ndarray) -> Tuple[str, float]:\n",
    "        try:\n",
    "            if self.model_path.suffix == '.pkl':\n",
    "                probabilities = self.model.predict_proba(features)[0]\n",
    "                \n",
    "            elif self.model_path.suffix == '.h5':\n",
    "                predictions = self.model.predict(features, verbose=0)[0]\n",
    "                \n",
    "                probabilities = predictions\n",
    "                if predictions.max() > 1.0 or predictions.min() < 0.0:\n",
    "                    exp_preds = np.exp(predictions - predictions.max())\n",
    "                    probabilities = exp_preds / exp_preds.sum()\n",
    "            \n",
    "            top_idx = int(np.argmax(probabilities))\n",
    "            predicted_label = self.class_names[top_idx]\n",
    "            confidence = float(probabilities[top_idx])\n",
    "            \n",
    "            return predicted_label, confidence\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Prediction failed: {e}\")\n",
    "            return \"unknown\", 0.0\n",
    "    \n",
    "    def _trigger_action(self, label: str) -> None:\n",
    "        label_lower = label.lower()\n",
    "        \n",
    "        if label_lower == \"next\":\n",
    "            pyautogui.press('right')\n",
    "            logger.info(f\"Action triggered: Next slide (right arrow)\")\n",
    "            \n",
    "        elif label_lower in [\"back\", \"previous\", \"prev\"]:\n",
    "            pyautogui.press('left')\n",
    "            logger.info(f\"Action triggered: Previous slide (left arrow)\")\n",
    "            \n",
    "        else:\n",
    "            logger.warning(f\"Unknown gesture label: {label}\")\n",
    "            return\n",
    "        \n",
    "        self.last_triggered_label = label\n",
    "        self.can_trigger = False\n",
    "        self.no_detection_count = 0\n",
    "    \n",
    "    def _update_trigger_state(self, detected: bool) -> None:\n",
    "        if not self.can_trigger:\n",
    "            if not detected:\n",
    "                self.no_detection_count += 1\n",
    "                \n",
    "                if self.no_detection_count >= self.disappear_frames:\n",
    "                    self.can_trigger = True\n",
    "                    self.no_detection_count = 0\n",
    "                    logger.info(\"Ready for next gesture trigger\")\n",
    "            else:\n",
    "                self.no_detection_count = 0\n",
    "    \n",
    "    def _detect_hand_location(self, frame: np.ndarray) -> Optional[Tuple[int, int, int, int]]:\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        lower_skin = np.array([0, 20, 70], dtype=np.uint8)\n",
    "        upper_skin = np.array([20, 255, 255], dtype=np.uint8)\n",
    "        mask = cv2.inRange(hsv, lower_skin, upper_skin)\n",
    "        \n",
    "        kernel = np.ones((5, 5), np.uint8)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)\n",
    "        mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)\n",
    "        \n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        if not contours:\n",
    "            return None\n",
    "        \n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        if cv2.contourArea(largest_contour) < 5000:\n",
    "            return None\n",
    "        \n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        return (x, y, w, h)\n",
    "    \n",
    "    def _draw_ui(\n",
    "        self,\n",
    "        frame: np.ndarray,\n",
    "        label: str,\n",
    "        confidence: float,\n",
    "        show_box: bool\n",
    "    ) -> np.ndarray:\n",
    "        if show_box and self.hand_bbox is not None:\n",
    "            x, y, w, h = self.hand_bbox\n",
    "            \n",
    "            color = (0, 255, 0)\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "            \n",
    "            label_text = f\"{label.capitalize()} {confidence*100:.1f}%\"\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                label_text,\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                2\n",
    "            )\n",
    "            \n",
    "            text_x = x\n",
    "            text_y = y - 10\n",
    "            \n",
    "            if text_y < text_height:\n",
    "                text_y = y + h + text_height + 10\n",
    "            \n",
    "            cv2.rectangle(\n",
    "                frame,\n",
    "                (text_x, text_y - text_height - 5),\n",
    "                (text_x + text_width + 10, text_y + 5),\n",
    "                (0, 0, 0),\n",
    "                -1\n",
    "            )\n",
    "            \n",
    "            cv2.putText(\n",
    "                frame,\n",
    "                label_text,\n",
    "                (text_x + 5, text_y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7,\n",
    "                color,\n",
    "                2\n",
    "            )\n",
    "        \n",
    "        status_text = \"Ready\" if self.can_trigger else \"Cooldown\"\n",
    "        status_color = (0, 255, 0) if self.can_trigger else (0, 165, 255)\n",
    "        \n",
    "        cv2.putText(\n",
    "            frame,\n",
    "            f\"Status: {status_text}\",\n",
    "            (10, 30),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7,\n",
    "            status_color,\n",
    "            2\n",
    "        )\n",
    "        \n",
    "        return frame\n",
    "    \n",
    "    def run(self) -> None:\n",
    "        logger.info(\"Starting gesture controller. Press 'q' to quit.\")\n",
    "        logger.info(f\"Confidence threshold: {self.confidence_threshold}\")\n",
    "        logger.info(f\"Cooldown frames: {self.disappear_frames}\")\n",
    "        \n",
    "        try:\n",
    "            while True:\n",
    "                ret, frame = self.cap.read()\n",
    "                \n",
    "                if not ret:\n",
    "                    logger.error(\"Failed to grab frame\")\n",
    "                    break\n",
    "                \n",
    "                self.frame_count += 1\n",
    "                \n",
    "                try:\n",
    "                    self.hand_bbox = self._detect_hand_location(frame)\n",
    "                    \n",
    "                    features = self._preprocess_roi(frame)\n",
    "                    label, confidence = self._predict(features)\n",
    "                    \n",
    "                    detected = confidence >= self.confidence_threshold\n",
    "                    \n",
    "                    if detected and self.can_trigger:\n",
    "                        self._trigger_action(label)\n",
    "                    \n",
    "                    self._update_trigger_state(detected)\n",
    "                    \n",
    "                    frame = self._draw_ui(frame, label, confidence, detected)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Frame processing error: {e}\")\n",
    "                \n",
    "                cv2.imshow('Hand Gesture Controller', frame)\n",
    "                \n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    logger.info(\"Quit signal received\")\n",
    "                    break\n",
    "                    \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Interrupted by user\")\n",
    "            \n",
    "        finally:\n",
    "            self.cleanup()\n",
    "    \n",
    "    def cleanup(self) -> None:\n",
    "        logger.info(\"Cleaning up resources...\")\n",
    "        \n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        \n",
    "        cv2.destroyAllWindows()\n",
    "        \n",
    "        logger.info(f\"Total frames processed: {self.frame_count}\")\n",
    "        logger.info(\"Shutdown complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model found: c:\\Users\\Arya\\Downloads\\HandGesturePresentationController-main\\HandGesturePresentationController-main\\artifacts\\gesture_svm.pkl\n",
      "Model type: .pkl\n"
     ]
    }
   ],
   "source": [
    "base_dir = Path.cwd()\n",
    "artifacts_dir = base_dir / \"artifacts\"\n",
    "\n",
    "model_path = artifacts_dir / \"gesture_svm.pkl\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    model_path = artifacts_dir / \"model.h5\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    print(f\"ERROR: No model found in {artifacts_dir}\")\n",
    "    print(\"Please ensure either gesture_svm.pkl or model.h5 exists\")\n",
    "else:\n",
    "    print(f\"Model found: {model_path}\")\n",
    "    print(f\"Model type: {model_path.suffix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run_header",
   "metadata": {},
   "source": [
    "## Run Gesture Controller\n",
    "\n",
    "**Instructions:**\n",
    "- The webcam window will open\n",
    "- Show hand gesture anywhere in the camera frame\n",
    "- Show \"Next\" gesture → presses right arrow key\n",
    "- Show \"Back\"/\"Previous\" gesture → presses left arrow key\n",
    "- Detection box will appear around detected gestures with confidence\n",
    "- After a gesture is triggered, remove your hand for 5 frames before the next trigger\n",
    "- Press 'q' to quit\n",
    "\n",
    "**Parameters:**\n",
    "- `confidence_threshold`: 0.65 (65% confidence required)\n",
    "- `disappear_frames`: 5 (hand must disappear for 5 frames)\n",
    "- `roi_bounds`: (0, 0, 640, 480) - full camera frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "run_controller",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-08 13:08:06,604 - __main__ - INFO - Loading SVM model from c:\\Users\\Arya\\Downloads\\HandGesturePresentationController-main\\HandGesturePresentationController-main\\artifacts\\gesture_svm.pkl\n",
      "2025-12-08 13:08:06,611 - __main__ - INFO - Model loaded successfully. Classes: ['next', 'previous']\n",
      "2025-12-08 13:08:06,613 - __main__ - INFO - Target input size: (128, 128)\n",
      "2025-12-08 13:08:18,230 - __main__ - INFO - Webcam initialized successfully\n",
      "2025-12-08 13:08:18,232 - __main__ - INFO - Starting gesture controller. Press 'q' to quit.\n",
      "2025-12-08 13:08:18,233 - __main__ - INFO - Confidence threshold: 0.65\n",
      "2025-12-08 13:08:18,234 - __main__ - INFO - Cooldown frames: 5\n",
      "2025-12-08 13:08:18,641 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:08:18,933 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:11:30,873 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:11:31,699 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:11:34,284 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:11:34,999 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:15,496 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:15,856 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:15,999 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:17,168 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:17,765 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:18,141 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:18,308 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:18,457 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:18,699 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:19,011 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:20,830 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:20,953 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:21,413 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:21,560 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:22,245 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:22,529 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:13:30,692 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:13:30,897 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:14:14,820 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:14:15,312 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:14:15,481 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:14:15,762 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:14:15,958 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:14:16,097 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:14:31,669 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:14:34,142 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:21,784 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:22,137 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:23,123 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:23,233 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:23,862 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:24,029 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:24,965 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:27,954 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:29,125 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:32,642 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:37,058 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:37,173 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:37,429 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:41,131 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:43,799 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:43,997 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:44,134 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:44,270 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:44,458 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:48,529 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:51,302 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:51,511 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:52,249 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:52,369 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:52,560 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:52,693 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:56,340 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:56,498 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:56,705 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:57,135 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:15:58,719 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:15:59,007 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:00,038 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:00,542 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:00,851 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:00,956 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:01,219 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:01,423 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:01,685 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:02,164 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:02,295 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:02,438 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:02,573 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:07,147 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:10,838 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:13,964 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:14,086 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:14,632 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:14,757 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:16,659 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:16,975 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:17,500 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:17,668 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:17,781 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:18,536 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:18,690 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:18,968 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:19,219 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:19,582 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:19,752 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:20,312 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:20,608 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:20,742 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:21,067 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:21,416 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:21,569 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:22,050 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:22,168 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:22,422 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:22,558 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:25,078 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:25,234 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:25,447 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:26,273 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:26,404 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:26,640 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:27,449 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:27,796 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:28,180 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:28,501 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:28,759 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:29,707 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:29,864 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:29,976 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:30,325 - __main__ - INFO - Action triggered: Previous slide (left arrow)\n",
      "2025-12-08 13:16:31,196 - __main__ - INFO - Ready for next gesture trigger\n",
      "2025-12-08 13:16:31,574 - __main__ - INFO - Quit signal received\n",
      "2025-12-08 13:16:31,575 - __main__ - INFO - Cleaning up resources...\n",
      "2025-12-08 13:16:31,909 - __main__ - INFO - Total frames processed: 14734\n",
      "2025-12-08 13:16:31,912 - __main__ - INFO - Shutdown complete\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    controller = GestureController(\n",
    "        model_path=model_path,\n",
    "        confidence_threshold=0.65,\n",
    "        disappear_frames=5,\n",
    "        roi_bounds=(0, 0, 640, 480)\n",
    "    )\n",
    "    \n",
    "    controller.run()\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Fatal error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computer_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
