{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "411e856f",
   "metadata": {},
   "source": [
    "# Hand Gesture Presentation Controller Pipeline\n",
    "\n",
    "This notebook implements the five-phase plan captured in `pipeline.md` so we can go from Roboflow Pascal VOC annotations to a deployable hand-gesture presentation controller.\n",
    "\n",
    "- **Phase 1:** Parse VOC XML files and crop gesture ROIs into a clean dataset structure.\n",
    "- **Phase 2:** Convert crops into fixed-length HOG descriptors.\n",
    "- **Phase 3:** Train and evaluate a conventional classifier (linear/RBF SVM).\n",
    "- **Phase 4:** Connect the classifier to a webcam ROI + `pyautogui` control loop.\n",
    "- **Phase 5:** Collect artifacts for the written report and demo video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a695e078",
   "metadata": {},
   "source": [
    "## Workspace Checklist\n",
    "- Code runs from the repo root: `HandGesturePresentationController/`.\n",
    "- Raw Pascal VOC data lives under `Dataset_roboflow/{train,valid,test}`.\n",
    "- Cropped conventional dataset will be stored in `dataset_final/{split}/{label}` (mirrors Phase 1 of the pipeline).\n",
    "- Fallback conventional data (if any) can be read from the legacy `Dataset/{Back,Next}` directories.\n",
    "- Artifacts (CSV manifests, trained models, charts) are saved inside `artifacts/` for report inclusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d0ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run once if the environment is missing any of these dependencies.\n",
    "# %pip install -q numpy pandas scikit-image scikit-learn matplotlib opencv-python pillow joblib pyautogui seaborn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f8806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\n",
      "Roboflow dataset: True | Processed dataset: True\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Sequence, Tuple\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage.feature import hog\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "import joblib\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "pd.set_option(\"display.max_rows\", 10)\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "ROBOFLOW_DATASET = BASE_DIR / \"Dataset_roboflow\"\n",
    "PROCESSED_DATASET = BASE_DIR / \"dataset_final\"\n",
    "FALLBACK_DATASET = BASE_DIR / \"Dataset\"\n",
    "ARTIFACTS_DIR = BASE_DIR / \"artifacts\"\n",
    "MODEL_BUNDLE_PATH = ARTIFACTS_DIR / \"gesture_svm_v2.pkl\"\n",
    "\n",
    "SPLITS: Sequence[str] = (\"train\", \"valid\", \"test\")\n",
    "ALLOWED_IMAGE_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\"}\n",
    "TARGET_IMAGE_SIZE: Tuple[int, int] = (128, 128)\n",
    "HOG_PARAMS: Dict[str, object] = {\n",
    "    \"orientations\": 9,\n",
    "    \"pixels_per_cell\": (16, 16),\n",
    "    \"cells_per_block\": (2, 2),\n",
    "    \"transform_sqrt\": True,\n",
    "    \"block_norm\": \"L2-Hys\",\n",
    "    \"feature_vector\": True,\n",
    "}\n",
    "LABEL_MAP = {\"next\": \"next\", \"back\": \"previous\", \"prev\": \"previous\"}\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DATASET.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {BASE_DIR}\")\n",
    "print(f\"Roboflow dataset: {ROBOFLOW_DATASET.exists()} | Processed dataset: {PROCESSED_DATASET.exists()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c3d909",
   "metadata": {},
   "source": [
    "## Phase 1 - Data Pre-processing (XML Parsing + Cropping)\n",
    "Goals:\n",
    "- Parse every Pascal VOC XML from Roboflow (`Dataset_roboflow`) and extract bounding boxes.\n",
    "- Normalize labels (`next`, `back` -> `previous`).\n",
    "- Crop the gesture region, optionally adding a small padding, and save it under `dataset_final/{split}/{label}` as required by `pipeline.md`.\n",
    "- Produce a CSV manifest that can be cited in the report (counts per split/label, augmentation notes, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_annotation_file(xml_path: Path) -> List[Dict[str, int]]:\n",
    "    \"\"\"Return all bounding boxes defined in a Pascal VOC XML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(xml_path)\n",
    "    except ET.ParseError as exc:\n",
    "        raise ValueError(f\"Failed to parse {xml_path}: {exc}\") from exc\n",
    "\n",
    "    root = tree.getroot()\n",
    "    objects: List[Dict[str, int]] = []\n",
    "    for obj in root.findall(\"object\"):\n",
    "        raw_label = obj.findtext(\"name\", default=\"\").strip().lower()\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        if not raw_label or bbox is None:\n",
    "            continue\n",
    "        xmin = int(float(bbox.findtext(\"xmin\", default=\"0\")))\n",
    "        xmax = int(float(bbox.findtext(\"xmax\", default=\"0\")))\n",
    "        ymin = int(float(bbox.findtext(\"ymin\", default=\"0\")))\n",
    "        ymax = int(float(bbox.findtext(\"ymax\", default=\"0\")))\n",
    "        objects.append({\"label\": raw_label, \"xmin\": xmin, \"ymin\": ymin, \"xmax\": xmax, \"ymax\": ymax})\n",
    "    return objects\n",
    "\n",
    "\n",
    "def resolve_image_for_xml(xml_path: Path) -> Path:\n",
    "    \"\"\"Find the image file that shares the same stem as the XML annotation.\"\"\"\n",
    "    candidates = [xml_path.with_suffix(ext) for ext in (\".jpg\", \".jpeg\", \".png\", \".bmp\")]\n",
    "    for candidate in candidates:\n",
    "        if candidate.exists():\n",
    "            return candidate\n",
    "    raise FileNotFoundError(f\"No image file found for {xml_path}\")\n",
    "\n",
    "\n",
    "def collect_crop_jobs(split_dir: Path) -> List[Dict[str, object]]:\n",
    "    jobs: List[Dict[str, object]] = []\n",
    "    for xml_path in sorted(split_dir.glob(\"*.xml\")):\n",
    "        image_path = resolve_image_for_xml(xml_path)\n",
    "        for obj in parse_annotation_file(xml_path):\n",
    "            normalized_label = LABEL_MAP.get(obj[\"label\"], None)\n",
    "            if normalized_label is None:\n",
    "                continue\n",
    "            jobs.append(\n",
    "                {\n",
    "                    \"split\": split_dir.name,\n",
    "                    \"label\": normalized_label,\n",
    "                    \"image_path\": image_path,\n",
    "                    \"xmin\": obj[\"xmin\"],\n",
    "                    \"ymin\": obj[\"ymin\"],\n",
    "                    \"xmax\": obj[\"xmax\"],\n",
    "                    \"ymax\": obj[\"ymax\"],\n",
    "                }\n",
    "            )\n",
    "    return jobs\n",
    "\n",
    "\n",
    "def crop_hand(job: Dict[str, object], output_root: Path, pad_ratio: float = 0.08) -> Path:\n",
    "    image = Image.open(job[\"image_path\"]).convert(\"RGB\")\n",
    "    width, height = image.size\n",
    "    w = job[\"xmax\"] - job[\"xmin\"]\n",
    "    h = job[\"ymax\"] - job[\"ymin\"]\n",
    "    pad_w = int(w * pad_ratio)\n",
    "    pad_h = int(h * pad_ratio)\n",
    "    x0 = max(0, job[\"xmin\"] - pad_w)\n",
    "    y0 = max(0, job[\"ymin\"] - pad_h)\n",
    "    x1 = min(width, job[\"xmax\"] + pad_w)\n",
    "    y1 = min(height, job[\"ymax\"] + pad_h)\n",
    "    crop = image.crop((x0, y0, x1, y1))\n",
    "\n",
    "    dest_dir = output_root / job[\"split\"] / job[\"label\"]\n",
    "    dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "    dest_path = dest_dir / f\"{job['split']}_{job['label']}_{job['image_path'].stem}_{x0}_{y0}_{x1}_{y1}.jpg\"\n",
    "    crop.save(dest_path)\n",
    "    return dest_path\n",
    "\n",
    "\n",
    "def run_phase_one(execute: bool = False, pad_ratio: float = 0.08) -> pd.DataFrame:\n",
    "    \"\"\"Optionally crop all annotations and return a manifest DataFrame.\"\"\"\n",
    "    all_jobs: List[Dict[str, object]] = []\n",
    "    for split in SPLITS:\n",
    "        split_dir = ROBOFLOW_DATASET / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        all_jobs.extend(collect_crop_jobs(split_dir))\n",
    "\n",
    "    if not all_jobs:\n",
    "        raise FileNotFoundError(\"No Pascal VOC annotations were found. Verify Dataset_roboflow/ is populated.\")\n",
    "\n",
    "    print(f\"Located {len(all_jobs)} bounding boxes across {len(set(job['split'] for job in all_jobs))} splits.\")\n",
    "    if not execute:\n",
    "        print(\"Dry run mode: set execute=True to create/refresh dataset_final/.\")\n",
    "        return pd.DataFrame(all_jobs)\n",
    "\n",
    "    cropped_records: List[Dict[str, object]] = []\n",
    "    for job in all_jobs:\n",
    "        dest_path = crop_hand(job, PROCESSED_DATASET, pad_ratio=pad_ratio)\n",
    "        record = {**job, \"crop_path\": dest_path.as_posix()}\n",
    "        cropped_records.append(record)\n",
    "\n",
    "    manifest = pd.DataFrame(cropped_records)\n",
    "    manifest.to_csv(ARTIFACTS_DIR / \"crop_manifest.csv\", index=False)\n",
    "    print(f\"Saved {len(manifest)} crops -> {ARTIFACTS_DIR / 'crop_manifest.csv'}\")\n",
    "    return manifest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba129dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Located 195 bounding boxes across 3 splits.\n",
      "Dry run mode: set execute=True to create/refresh dataset_final/.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>image_path</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train</td>\n",
       "      <td>next</td>\n",
       "      <td>c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...</td>\n",
       "      <td>86</td>\n",
       "      <td>240</td>\n",
       "      <td>492</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train</td>\n",
       "      <td>next</td>\n",
       "      <td>c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...</td>\n",
       "      <td>86</td>\n",
       "      <td>240</td>\n",
       "      <td>492</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train</td>\n",
       "      <td>next</td>\n",
       "      <td>c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...</td>\n",
       "      <td>86</td>\n",
       "      <td>240</td>\n",
       "      <td>492</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train</td>\n",
       "      <td>next</td>\n",
       "      <td>c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...</td>\n",
       "      <td>130</td>\n",
       "      <td>152</td>\n",
       "      <td>357</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train</td>\n",
       "      <td>next</td>\n",
       "      <td>c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...</td>\n",
       "      <td>130</td>\n",
       "      <td>152</td>\n",
       "      <td>357</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   split label                                         image_path  xmin  ymin  \\\n",
       "0  train  next  c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...    86   240   \n",
       "1  train  next  c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...    86   240   \n",
       "2  train  next  c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...    86   240   \n",
       "3  train  next  c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...   130   152   \n",
       "4  train  next  c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGe...   130   152   \n",
       "\n",
       "   xmax  ymax  \n",
       "0   492   393  \n",
       "1   492   393  \n",
       "2   492   393  \n",
       "3   357   311  \n",
       "4   357   311  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CROP_DATA = False  # Switch to True when you are ready to refresh dataset_final/\n",
    "phase_one_manifest = run_phase_one(execute=CROP_DATA, pad_ratio=0.08)\n",
    "\n",
    "if phase_one_manifest is not None:\n",
    "    display(phase_one_manifest.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d48b1",
   "metadata": {},
   "source": [
    "## Phase 2 - Feature Extraction (Grayscale + Resize + HOG)\n",
    "Goals:\n",
    "- Load every cropped image, convert to grayscale, and resize to the fixed `128x128` target required by SVM.\n",
    "- Extract HOG descriptors using the parameters defined in `HOG_PARAMS`.\n",
    "- Persist the feature matrix (NumPy `.npz`) and a metadata table (CSV) so we can reuse them without recomputing.\n",
    "- Produce quick sanity-check visuals/statistics for the written report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626613f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_has_images(root: Path) -> bool:\n",
    "    if not root.exists():\n",
    "        return False\n",
    "    return any(root.rglob(\"*.jpg\")) or any(root.rglob(\"*.png\"))\n",
    "\n",
    "\n",
    "def resolve_feature_dataset_root(preferred: Path, fallback: Path) -> Path:\n",
    "    if dataset_has_images(preferred):\n",
    "        return preferred\n",
    "    if dataset_has_images(fallback):\n",
    "        print(f\"Warning: {preferred} is empty; using fallback {fallback}\")\n",
    "        return fallback\n",
    "    raise FileNotFoundError(\"No cropped dataset found. Run Phase 1 or place images under Dataset/.\")\n",
    "\n",
    "\n",
    "def iter_image_paths(dataset_root: Path, splits: Sequence[str] | None = None):\n",
    "    has_split_structure = any((dataset_root / split).exists() for split in SPLITS)\n",
    "    if has_split_structure:\n",
    "        target_splits = splits if splits else [p.name for p in dataset_root.iterdir() if p.is_dir()]\n",
    "        for split in target_splits:\n",
    "            split_dir = dataset_root / split\n",
    "            if not split_dir.exists():\n",
    "                continue\n",
    "            for label_dir in sorted(p for p in split_dir.iterdir() if p.is_dir()):\n",
    "                for image_path in sorted(label_dir.iterdir()):\n",
    "                    if image_path.suffix.lower() in ALLOWED_IMAGE_EXTS:\n",
    "                        yield split, label_dir.name.lower(), image_path\n",
    "    else:\n",
    "        for label_dir in sorted(p for p in dataset_root.iterdir() if p.is_dir()):\n",
    "            for image_path in sorted(label_dir.iterdir()):\n",
    "                if image_path.suffix.lower() in ALLOWED_IMAGE_EXTS:\n",
    "                    yield \"all\", label_dir.name.lower(), image_path\n",
    "\n",
    "\n",
    "def compute_hog_descriptor(image_path: Path, target_size: Tuple[int, int], hog_params: Dict[str, object]) -> np.ndarray:\n",
    "    image = Image.open(image_path).convert(\"L\").resize(target_size)\n",
    "    image_arr = np.asarray(image, dtype=np.float32) / 255.0\n",
    "    return hog(image_arr, **hog_params)\n",
    "\n",
    "\n",
    "def build_feature_set(\n",
    "    dataset_root: Path,\n",
    "    target_size: Tuple[int, int],\n",
    "    hog_params: Dict[str, object],\n",
    "    splits: Sequence[str] | None = None,\n",
    "    persist: bool = True,\n",
    ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "    records: List[Dict[str, object]] = []\n",
    "    feature_rows: List[np.ndarray] = []\n",
    "\n",
    "    for split_name, label_name, image_path in iter_image_paths(dataset_root, splits=splits):\n",
    "        descriptor = compute_hog_descriptor(image_path, target_size, hog_params)\n",
    "        feature_rows.append(descriptor)\n",
    "        records.append({\n",
    "            \"split\": split_name,\n",
    "            \"label\": LABEL_MAP.get(label_name, label_name),\n",
    "            \"image_path\": image_path.as_posix(),\n",
    "        })\n",
    "\n",
    "    if not feature_rows:\n",
    "        raise ValueError(f\"No eligible images found under {dataset_root}\")\n",
    "\n",
    "    feature_matrix = np.vstack(feature_rows)\n",
    "    meta_df = pd.DataFrame(records)\n",
    "\n",
    "    if persist:\n",
    "        np.savez_compressed(ARTIFACTS_DIR / \"hog_features.npz\", X=feature_matrix, y=meta_df[\"label\"].values)\n",
    "        meta_df.to_csv(ARTIFACTS_DIR / \"hog_metadata.csv\", index=False)\n",
    "        print(f\"Persisted features -> {ARTIFACTS_DIR / 'hog_features.npz'}\")\n",
    "        print(f\"Persisted metadata -> {ARTIFACTS_DIR / 'hog_metadata.csv'}\")\n",
    "\n",
    "    return feature_matrix, meta_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e072d4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\\dataset_final is empty; using fallback c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\\Dataset\n",
      "Using processed dataset at: c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\\Dataset\n",
      "Persisted features -> c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\\artifacts\\hog_features.npz\n",
      "Persisted metadata -> c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\\artifacts\\hog_metadata.csv\n",
      "Feature matrix shape: (82, 1764)\n",
      "Label distribution:\n",
      " label\n",
      "previous    41\n",
      "next        41\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>split</th>\n",
       "      <th>label</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all</td>\n",
       "      <td>previous</td>\n",
       "      <td>c:/Users/ASUS/Documents/Computer Vision/HandGe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all</td>\n",
       "      <td>previous</td>\n",
       "      <td>c:/Users/ASUS/Documents/Computer Vision/HandGe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all</td>\n",
       "      <td>previous</td>\n",
       "      <td>c:/Users/ASUS/Documents/Computer Vision/HandGe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>all</td>\n",
       "      <td>previous</td>\n",
       "      <td>c:/Users/ASUS/Documents/Computer Vision/HandGe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>all</td>\n",
       "      <td>previous</td>\n",
       "      <td>c:/Users/ASUS/Documents/Computer Vision/HandGe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  split     label                                         image_path\n",
       "0   all  previous  c:/Users/ASUS/Documents/Computer Vision/HandGe...\n",
       "1   all  previous  c:/Users/ASUS/Documents/Computer Vision/HandGe...\n",
       "2   all  previous  c:/Users/ASUS/Documents/Computer Vision/HandGe...\n",
       "3   all  previous  c:/Users/ASUS/Documents/Computer Vision/HandGe...\n",
       "4   all  previous  c:/Users/ASUS/Documents/Computer Vision/HandGe..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dataset_root = resolve_feature_dataset_root(PROCESSED_DATASET, FALLBACK_DATASET)\n",
    "print(f\"Using processed dataset at: {feature_dataset_root}\")\n",
    "\n",
    "X, feature_meta = build_feature_set(feature_dataset_root, TARGET_IMAGE_SIZE, HOG_PARAMS, persist=True)\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(\"Label distribution:\\n\", feature_meta[\"label\"].value_counts())\n",
    "\n",
    "feature_meta.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d878e3fd",
   "metadata": {},
   "source": [
    "## Phase 3 - Model Training + Evaluation (SVM)\n",
    "Goals:\n",
    "- Split the HOG feature matrix into train/test (default 80/20 with stratification).\n",
    "- Train both linear and RBF-flavored SVMs; keep the best-performing option.\n",
    "- Report accuracy, precision, recall, and a confusion matrix for the written report.\n",
    "- Persist the trained model (`gesture_svm.pkl`) along with preprocessing metadata for later inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e9445b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   kernel  accuracy\n",
      "0  linear  0.588235\n",
      "1     rbf  0.647059\n",
      "Selected kernel: rbf\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        next       0.67      0.67      0.67         9\n",
      "    previous       0.62      0.62      0.62         8\n",
      "\n",
      "    accuracy                           0.65        17\n",
      "   macro avg       0.65      0.65      0.65        17\n",
      "weighted avg       0.65      0.65      0.65        17\n",
      "\n",
      "Saved model bundle -> c:\\Users\\ASUS\\Documents\\Computer Vision\\HandGesturePresentationController\\artifacts\\gesture_svm.pkl\n"
     ]
    }
   ],
   "source": [
    "def train_and_select_svm(\n",
    "    features: np.ndarray,\n",
    "    labels: Sequence[str],\n",
    "    test_size: float = 0.2,\n",
    "    random_state: int = RANDOM_STATE,\n",
    "):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        labels,\n",
    "        test_size=test_size,\n",
    "        stratify=labels,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    kernel_grid = [\n",
    "        {\"name\": \"linear\", \"params\": {\"kernel\": \"linear\", \"C\": 1.0}},\n",
    "        {\"name\": \"rbf\", \"params\": {\"kernel\": \"rbf\", \"C\": 5.0, \"gamma\": \"scale\"}},\n",
    "    ]\n",
    "\n",
    "    best_result = None\n",
    "    evaluations = []\n",
    "\n",
    "    for cfg in kernel_grid:\n",
    "        clf = SVC(probability=True, class_weight=\"balanced\", random_state=random_state, **cfg[\"params\"])\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred, zero_division=0)\n",
    "        eval_result = {\n",
    "            \"kernel\": cfg[\"name\"],\n",
    "            \"accuracy\": acc,\n",
    "            \"report\": report,\n",
    "            \"model\": clf,\n",
    "            \"y_pred\": y_pred,\n",
    "        }\n",
    "        evaluations.append(eval_result)\n",
    "        if best_result is None or acc > best_result[\"accuracy\"]:\n",
    "            best_result = {**eval_result, \"X_test\": X_test, \"y_test\": y_test}\n",
    "\n",
    "    return best_result, evaluations, (X_train, X_test, y_train, y_test)\n",
    "\n",
    "\n",
    "best_model_result, eval_summary, splits_cache = train_and_select_svm(X, feature_meta[\"label\"].values)\n",
    "print(pd.DataFrame(eval_summary)[[\"kernel\", \"accuracy\"]])\n",
    "print(\"Selected kernel:\", best_model_result[\"kernel\"])\n",
    "print(best_model_result[\"report\"])\n",
    "\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": best_model_result[\"model\"],\n",
    "        \"hog_params\": HOG_PARAMS,\n",
    "        \"target_size\": TARGET_IMAGE_SIZE,\n",
    "        \"label_map\": LABEL_MAP,\n",
    "    },\n",
    "    MODEL_BUNDLE_PATH,\n",
    ")\n",
    "print(f\"Saved model bundle -> {MODEL_BUNDLE_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce3df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 450x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = confusion_matrix(best_model_result[\"y_test\"], best_model_result[\"y_pred\"], labels=best_model_result[\"model\"].classes_)\n",
    "fig, ax = plt.subplots(figsize=(4.5, 4.5))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=best_model_result[\"model\"].classes_)\n",
    "disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "ax.set_title(\"Gesture Classifier Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669c0125",
   "metadata": {},
   "source": [
    "## Phase 4 - Application Layer (Webcam ROI + Slide Control)\n",
    "Goals:\n",
    "- Use a fixed Region of Interest (ROI) on the webcam feed (green box strategy) per the pipeline.\n",
    "- Reuse the preprocessing steps (grayscale, resize, HOG) before calling the trained SVM.\n",
    "- Apply a confidence threshold + cooldown to debounce gestures before sending `pyautogui` keyboard events (`right` = next, `left` = previous).\n",
    "- Keep the loop modular so it can be imported into the final demo script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_roi_for_inference(frame, roi_bounds, hog_params, target_size):\n",
    "    x0, y0, x1, y1 = roi_bounds\n",
    "    roi_frame = frame[y0:y1, x0:x1]\n",
    "    gray = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, target_size)\n",
    "    normalized = resized.astype(\"float32\") / 255.0\n",
    "    descriptor = hog(normalized, **hog_params)\n",
    "    return descriptor.reshape(1, -1)\n",
    "\n",
    "\n",
    "from collections import deque\n",
    "import statistics\n",
    "\n",
    "def run_gesture_controller_smoothed(\n",
    "    model_path: Path = MODEL_BUNDLE_PATH,\n",
    "    roi_bounds: Tuple[int, int, int, int] = (200, 120, 440, 360),\n",
    "    confidence_threshold: float = 0.70,  # Bumped up slightly\n",
    "    cooldown_seconds: float = 2.0,\n",
    "    history_len: int = 5  # NEW: Number of frames to confirm gesture\n",
    "):\n",
    "    import pyautogui\n",
    "    import time\n",
    "    \n",
    "    # Load Model\n",
    "    bundle = joblib.load(model_path)\n",
    "    clf = bundle[\"model\"]\n",
    "    hog_params = bundle[\"hog_params\"]\n",
    "    target_size = tuple(bundle[\"target_size\"])\n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    last_trigger = 0.0\n",
    "    # History buffer to store the last 'history_len' predictions\n",
    "    prediction_history = deque(maxlen=history_len)\n",
    "    \n",
    "    print(\"--- LIVE DEMO STARTED ---\")\n",
    "    print(f\"ROI: {roi_bounds} | Smoothing: {history_len} frames\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok: break\n",
    "\n",
    "            # 1. Inference\n",
    "            descriptor = preprocess_roi_for_inference(frame, roi_bounds, hog_params, target_size)\n",
    "            proba = clf.predict_proba(descriptor)[0]\n",
    "            top_idx = int(np.argmax(proba))\n",
    "            current_label = clf.classes_[top_idx]\n",
    "            confidence = float(proba[top_idx])\n",
    "\n",
    "            # 2. Add to History (Only if confidence is high)\n",
    "            if confidence > confidence_threshold:\n",
    "                prediction_history.append(current_label)\n",
    "            else:\n",
    "                prediction_history.append(\"neutral\") # Noise/Low conf\n",
    "\n",
    "            # 3. Consensus Vote (The Smoothing Logic)\n",
    "            # Only trigger if the buffer is full AND all items are the same\n",
    "            if len(prediction_history) == history_len:\n",
    "                # Get the most common label in history\n",
    "                most_common = statistics.mode(prediction_history)\n",
    "                \n",
    "                # Check if it dominates the history (e.g. 5/5 frames)\n",
    "                if prediction_history.count(most_common) == history_len and most_common != \"neutral\":\n",
    "                    \n",
    "                    # 4. Trigger Action (Debounced)\n",
    "                    if (time.time() - last_trigger) >= cooldown_seconds:\n",
    "                        key = \"right\" if most_common == \"next\" else \"left\"\n",
    "                        pyautogui.press(key)\n",
    "                        last_trigger = time.time()\n",
    "                        print(f\">>> ACTION CONFIRMED: {most_common} (Confidence: {confidence:.2f})\")\n",
    "                        \n",
    "                        # Visual Feedback Flash\n",
    "                        cv2.rectangle(frame, (roi_bounds[0], roi_bounds[1]), \n",
    "                                    (roi_bounds[2], roi_bounds[3]), (0, 255, 0), -1)\n",
    "\n",
    "            # --- VISUALIZATION ---\n",
    "            # Draw Static ROI (Green Box)\n",
    "            cv2.rectangle(frame, (roi_bounds[0], roi_bounds[1]), \n",
    "                        (roi_bounds[2], roi_bounds[3]), (0, 255, 0), 2)\n",
    "            \n",
    "            # Show Raw Prediction\n",
    "            cv2.putText(frame, f\"Raw: {current_label} ({confidence:.2f})\", \n",
    "                      (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "            \n",
    "            # Show History Status\n",
    "            hist_str = \" \".join([p[0].upper() for p in prediction_history]) # Show N/P/N\n",
    "            cv2.putText(frame, f\"Hist: [{hist_str}]\", \n",
    "                      (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)\n",
    "\n",
    "            cv2.imshow(\"Gesture Control (Smoothed)\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# def run_gesture_controller(\n",
    "#     model_path: Path = MODEL_BUNDLE_PATH,\n",
    "#     roi_bounds: Tuple[int, int, int, int] = (200, 120, 440, 360),\n",
    "#     confidence_threshold: float = 0.65,\n",
    "#     cooldown_seconds: float = 2.0,\n",
    "# ):\n",
    "#     \"\"\"Live webcam loop that triggers PowerPoint/Keynote hotkeys via pyautogui.\"\"\"\n",
    "#     import pyautogui\n",
    "#     import time\n",
    "\n",
    "#     bundle = joblib.load(model_path)\n",
    "#     clf = bundle[\"model\"]\n",
    "#     hog_params = bundle[\"hog_params\"]\n",
    "#     target_size = tuple(bundle[\"target_size\"])\n",
    "\n",
    "#     cap = cv2.VideoCapture(0)\n",
    "#     if not cap.isOpened():\n",
    "#         raise RuntimeError(\"Unable to open webcam. Ensure a camera is connected and not in use.\")\n",
    "\n",
    "#     last_trigger = 0.0\n",
    "#     try:\n",
    "#         while True:\n",
    "#             ok, frame = cap.read()\n",
    "#             if not ok:\n",
    "#                 break\n",
    "\n",
    "#             descriptor = preprocess_roi_for_inference(frame, roi_bounds, hog_params, target_size)\n",
    "#             proba = clf.predict_proba(descriptor)[0]\n",
    "#             top_idx = int(np.argmax(proba))\n",
    "#             predicted_label = clf.classes_[top_idx]\n",
    "#             confidence = float(proba[top_idx])\n",
    "\n",
    "#             color = (0, 255, 0) if confidence >= confidence_threshold else (0, 165, 255)\n",
    "#             x0, y0, x1, y1 = roi_bounds\n",
    "#             cv2.rectangle(frame, (x0, y0), (x1, y1), color, 2)\n",
    "#             cv2.putText(\n",
    "#                 frame,\n",
    "#                 f\"{predicted_label} ({confidence:.2f})\",\n",
    "#                 (x0, y0 - 10),\n",
    "#                 cv2.FONT_HERSHEY_SIMPLEX,\n",
    "#                 0.7,\n",
    "#                 color,\n",
    "#                 2,\n",
    "#             )\n",
    "\n",
    "#             if confidence >= confidence_threshold and (time.time() - last_trigger) >= cooldown_seconds:\n",
    "#                 key_to_press = \"right\" if predicted_label == \"next\" else \"left\"\n",
    "#                 pyautogui.press(key_to_press)\n",
    "#                 last_trigger = time.time()\n",
    "#                 print(f\"Triggered {predicted_label} at confidence {confidence:.2f}\")\n",
    "\n",
    "#             cv2.imshow(\"Hand Gesture Controller\", frame)\n",
    "#             if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#                 break\n",
    "#     finally:\n",
    "#         cap.release()\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# Example usage (run only when you are ready for the live demo):\n",
    "# run_gesture_controller(roi_bounds=(220, 140, 460, 380))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c57b3",
   "metadata": {},
   "source": [
    "## Phase 5 - Reporting, Demo, and Next Steps\n",
    "- **Report assets:**\n",
    "  - Include `artifact/crop_manifest.csv`, `hog_metadata.csv`, and the confusion matrix figure. These back Methodology + Results sections (LO1-4 coverage).\n",
    "  - Document HOG hyperparameters, SVM settings, and ROI coordinates inside the report.\n",
    "- **Demo video checklist:**\n",
    "  - Show the green ROI overlay, explain the debounce logic, and demonstrate both `next` and `previous` triggers.\n",
    "  - Mention explicitly that only conventional models + HOG features are used (matches project constraints).\n",
    "- **Future experiments:** try additional descriptors (LBP, Hu moments), add data augmentation on crops, or tune SVM `C`/`gamma` via grid search for higher accuracy.\n",
    "- **Verification:** rerun Phases 1-4 whenever the dataset grows, then regenerate metrics for the final submission package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4800be5f",
   "metadata": {},
   "source": [
    "## Changes :\n",
    "#   - HOG Parameters:\n",
    "        - pixels_per_cell=(8, 8) -> pixels_per_cell=(16, 16)\n",
    "        - ~8,100 features -> Less\n",
    "        - Make more general, from before too detailed\n",
    "\n",
    "# Discussion:\n",
    "- **Why did Linear accuracy drop?** By increasing pixels_per_cell to 16x16, you blurred the fine details. The \"Linear\" boundary couldn't find a straight line to separate \"Next\" from \"Previous\" anymore because the data became \"clumpier.\"\n",
    "\n",
    "- **Why did RBF win?** The RBF kernel is non-linear. It is better at drawing curvy boundaries around these coarser, clumpier data points.\n",
    "\n",
    "- **The Reality:** 65% is still dangerously close to a coin flip (50%). The Smoothing Function (Deque) I gave you in Phase 4 is now the only thing keeping your demo from flickering wildly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clone_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
